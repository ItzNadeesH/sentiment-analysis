{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üé¨ IMDB Movie Review Sentiment Analysis ‚Äì Preprocessing"
      ],
      "metadata": {
        "id": "9UgIu8qf6U75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf6RWFGG6aaD",
        "outputId": "460197c5-f4db-4c20-a68b-61307d35574f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqII4Be253Xp",
        "outputId": "3659b239-a422-4cbb-a7d9-fe9f19cc4fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'imdb-dataset-of-50k-movie-reviews' dataset.\n",
            "Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import contractions"
      ],
      "metadata": {
        "id": "qvopWqp-6f4Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "# Download required resources for tokenize and remove stopwords\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download required resources for lemmatization and POS tagging\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIlsu1As6nvR",
        "outputId": "1c223ec0-4f23-426e-f2ed-bcfa25f08e14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dRUZ9RUP8Sw4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path + \"/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "yFo87WHE6tZt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "   # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # 3. Remove URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "    # 4. Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 5. Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 6. Remove extra whitespaces\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Z09rZrUS6v7j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "pSCgwHWb7Abo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_remove_stopwords(text):\n",
        "    # Tokenize\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "jc3AIO8U7Bj7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
        "    return lemmatized_words"
      ],
      "metadata": {
        "id": "cWtT6tMB7Dqk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_negations(tokens):\n",
        "    negation_words = {\"not\", \"no\", \"never\", \"none\", \"cannot\", \"n't\"}\n",
        "    new_tokens = []\n",
        "    negate = False\n",
        "\n",
        "    for word in tokens:\n",
        "        if word in negation_words:\n",
        "            negate = True\n",
        "            continue\n",
        "        if negate:\n",
        "            new_tokens.append(\"NOT_\" + word)\n",
        "            negate = False\n",
        "        else:\n",
        "            new_tokens.append(word)\n",
        "    return new_tokens"
      ],
      "metadata": {
        "id": "sX0XK_9b7GKQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the text: remove unwanted characters, punctuation, and make lowercase\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Expand contractions: e.g., \"can't\" ‚Üí \"cannot\", \"won't\" ‚Üí \"will not\"\n",
        "df['expanded_review'] = df['cleaned_review'].apply(expand_contractions)\n",
        "\n",
        "# Tokenize and remove stopwords: split sentences into words and remove common filler words\n",
        "df['tokens'] = df['expanded_review'].apply(tokenize_and_remove_stopwords)\n",
        "\n",
        "# Lemmatize tokens: reduce words to their base or dictionary form (e.g., \"running\" ‚Üí \"run\")\n",
        "df['lemmatized_tokens'] = df['tokens'].apply(lemmatize_tokens)\n",
        "\n",
        "# Handle negations: adjust token meanings around negation words (e.g., \"not good\" ‚Üí \"not_good\")\n",
        "df['final_tokens'] = df['lemmatized_tokens'].apply(handle_negations)\n",
        "\n",
        "# Move the 'sentiment' column to the end of the DataFrame\n",
        "if 'sentiment' in df.columns:\n",
        "    cols = [col for col in df.columns if col != 'sentiment'] + ['sentiment']\n",
        "    df = df[cols]"
      ],
      "metadata": {
        "id": "iEmmeIGO7LZC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed DataFrame to a CSV file\n",
        "df.to_csv('processed_reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "dih8RiP679HY"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}